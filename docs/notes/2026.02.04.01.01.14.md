You’re not crazy for wanting “document-first + portable + queryable + multiple targets.” That’s a **data lake** shape, not an app-db shape.

And yeah: **DuckDB is a very strong fit** for the *query layer* on top of a document/object store — with two big bonuses:

* **FTS extension** (keyword / full-text search) ([DuckDB][1])
* **VSS extension** (vector similarity search via HNSW) — *but it’s explicitly experimental* and has persistence / RAM caveats ([DuckDB][2])

---

## The architecture that matches what you just said

### 1) **Write path = document/object store**

Keep ingestion dead simple and vendor-neutral:

* **Events**: append-only **JSONL** (one JSON doc per line)
* **Attachments/blobs**: content-addressed files (sha256 path)
* **Optional**: mirror into MongoDB for “live app” queries if you want

This gives you “fluid schema” with a tiny fixed core.

### 2) **Query path = DuckDB**

DuckDB becomes your “query engine overlay”:

* Reads JSONL directly, or (better) reads **Parquet**
* Builds **FTS indexes** for fast text search ([DuckDB][1])
* Can do **vector search** using `vss` if you want one engine — but treat it as optional/experimental ([DuckDB][2])

This is the “queryable data lake export target” you described — without committing to one vendor DB.

---

## A core schema that stays stable while everything else stays fluid

Think “strict envelope + freeform payload”.

### **Event envelope**

* `id` (stable UUID or hash)
* `ts` (ISO timestamp)
* `source` (`opencode` | `chatgpt` | `perplexity` | …)
* `source_ref` (session id / conversation id / message id)
* `kind` (`message` | `tool_call` | `tool_result` | `snapshot` | `attachment` | `derived`)
* `text` (normalized text for search)
* `attachments` (array of `{blob_hash, mime, name, size}`)
* `meta` (small standardized map: role, author, model, project, tags)
* `extra` (anything else, unstructured)

That’s enough to unify ChatGPT history + OpenCode sessions while allowing the differences to live in `extra`.

---

## Where MongoDB fits (and where it hurts)

Mongo is great as an **operational store** (live dashboards, recent activity, queue state, job tracking).

Mongo is not as great as the **canonical archive** for your “export target / data lake” goals because:

* portability is weaker than “folder of parquet/jsonl + blobs”
* analytics queries are just nicer in DuckDB

So: keep Mongo if you want it, but treat it as a **cache/materialized view**, not the source of truth.

---

## DuckDB vs Chroma for vectors

If you want *one* system: DuckDB can do vector similarity search via `vss` with HNSW indexes ([DuckDB][2])

But: the DuckDB docs repeatedly frame it as **experimental**, with limitations like RAM residency and persistence behaviors. ([DuckDB][2])

So the pragmatic choice is:

* **MVP**: DuckDB for FTS + analytics, and keep embeddings in Parquet (no index) or in Chroma
* **Later**: try DuckDB `vss` if you want to collapse stack complexity

Also worth noting: DuckDB has an official **FTS extension** that’s very SQLite-FTS-like. ([DuckDB][1])

---

## How this becomes an OpenCode addition (not a fork)

Ship a plugin that does **export + index**, not “replace OpenCode.”

### Plugin responsibilities

1. Subscribe to session/message events
2. Write normalized JSONL events + blobs
3. Periodically compact into Parquet partitions
4. Maintain a DuckDB file for indexes (FTS now; VSS later)

### Tools the plugin exposes to the LLM

* `planner.search_fts(query, limit)`
* `planner.get_session(session_id)`
* `planner.compile_pack(target)` → generates `SPEC/DECISIONS/TODO/MANIFEST`
* `planner.import_chatgpt_export(zip)` (optional MVP2)

This keeps OpenCode as “coding harness,” while giving it a **cross-client session knowledge layer**.

---

## Why this solves your “discoverability” + “zip chaos”

You stop treating “zips” as identity.

The canonical archive becomes:

* **event log** (JSONL/Parquet)
* **blob store** (attachments + extracted code blocks + snapshot content if you want)
* **indices** (DuckDB FTS, maybe VSS)

Then “best version of conversation” is a **compiled pack** stored as a derived artifact event, not another zip with a slightly different name.

---

## If you want a crisp recommendation

### ✅ Yes: DuckDB

Use it as:

* portable query engine
* FTS index builder ([DuckDB][1])
* optional vector search engine (experimental) ([DuckDB][2])

### Canonical store

* JSONL + blobs (always)
* Parquet partitions (soon as you can)
* Mongo optional as a mirror

---

## Now-2 for shipping something people will use

### Now

Ship **OpenCode plugin MVP0**:

* exports sessions/messages to a local data lake:

  * `lake/events/YYYY/MM/DD/*.jsonl`
  * `lake/blobs/sha256/<hash>`
* builds a DuckDB `archive.duckdb` with:

  * a table `events` over your JSONL/parquet
  * FTS index on `events.text` ([DuckDB][1])
* exposes `planner.search_fts`

This is immediately useful and doesn’t require embeddings.

### Later

Add:

* ChatGPT export importer → same event format
* Embeddings pipeline → store vectors in Parquet
* Optional DuckDB `vss` index for semantic search ([DuckDB][2])

---

If you want, tell me one constraint and I’ll pick the exact on-disk layout:

* **Single-file portable** (everything in one folder you can zip/move)
* **Huge archive** (partitioned parquet, incremental compaction)
* **Offline first** (no services required)
* **API-first** (Fastify as primary interface)

#opencode #duckdb #datalake #search #rag #workflow

[1]: https://duckdb.org/docs/stable/core_extensions/full_text_search.html?utm_source=chatgpt.com "Full-Text Search Extension – DuckDB"
[2]: https://duckdb.org/docs/stable/core_extensions/vss.html?utm_source=chatgpt.com "Vector Similarity Search Extension – DuckDB"
